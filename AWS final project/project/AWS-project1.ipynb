{
  "metadata": {
    "vscode": {
      "interpreter": {
        "hash": "db884c9a7d7a283a0103bbb64d72c1b2a9d8a4070d6cfe92517e4a6a915bccb0"
      }
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\ns3_resource = boto3.resource(\"s3\")\ns3 = boto3.client('s3')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "cfn = boto3.client('cloudformation')\n\ndef get_cfn_outputs(stackname):\n    outputs = {}\n    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n        outputs[output['OutputKey']] = output['OutputValue']\n    return outputs\n\n## Setup variables to use for the rest of the demo\ncloudformation_stack_name = \"vis-search\"\n\noutputs = get_cfn_outputs(cloudformation_stack_name)\n\nbucket = outputs['s3BucketTraining']\nes_host = outputs['esHostName']\n\noutputs",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Data Preparation\n\nimport os \nimport json\nimport urllib.request\nfrom multiprocessing import cpu_count\nfrom tqdm.contrib.concurrent import process_map\n\nimages_path = 'data/feidegger/fashion'\nfilename = 'metadata.json'\n\nmy_bucket = s3_resource.Bucket(bucket)\n\nif not os.path.isdir(images_path):\n    os.makedirs(images_path)\n\ndef download_metadata(url):\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n        \n# download metadata.json to local notebook\ndownload_metadata('https://raw.githubusercontent.com/zalandoresearch/feidegger/master/data/FEIDEGGER_release_1.2.json')\n\ndef generate_image_list(filename):\n    metadata = open(filename,'r')\n    data = json.load(metadata)\n    url_lst = []\n    for i in data:\n        url_lst.append(i['url'])\n    return url_lst\n\n\ndef download_image(url):\n    urllib.request.urlretrieve(url, images_path + '/' + url.split(\"/\")[-1])\n                    \n# generate image list            \nurl_lst = generate_image_list(filename)     \n\nworkers = 2 * cpu_count()\n\n# downloading images to local disk; This process will take approximately 2-5 minutes on a t3.medium notebook instance\n_ = process_map(download_image, url_lst, max_workers=workers, chunksize=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Uploading dataset to S3\n\nfiles_to_upload = []\ndirName = 'data'\nfor path, subdirs, files in os.walk('./' + dirName):\n    path = path.replace(\"\\\\\",\"/\")\n    directory_name = path.replace('./',\"\")\n    for file in files:\n        files_to_upload.append({\n            \"filename\": os.path.join(path, file),\n            \"key\": directory_name+'/'+file\n        })\n\ndef upload_to_s3(file):\n    my_bucket.upload_file(file['filename'], file['key'])\n\n# uploading images to s3; This process will take approximately 2-5 minutes on a t3.medium notebook instance\n_ = process_map(upload_to_s3, files_to_upload, max_workers=workers, chunksize=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import tensorflow.keras as keras\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom PIL import Image",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "keras.backend.set_image_data_format(data_format='channels_last')\n\n# Import Resnet50 model\nmodel = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224,224,3))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Creating the directory strcture\ndirName = 'model/1'\nif not os.path.exists(dirName):\n    os.makedirs(dirName)\n    print(\"Directory \" , dirName ,  \" Created \")\nelse:\n    print(\"Directory \" , dirName ,  \" already exists\")    ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "%time\n# Save the model in SavedModel format\nmodel.save('./model/1/', save_format='tf')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Check the model Signature\n!/home/ec2-user/anaconda3/envs/tensorflow2_p38/bin/saved_model_cli show --dir ./model/1/ --tag_set serve --signature_def serving_default",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import tarfile\n\n# zip the model .gz format\nmodel_version = '1'\nexport_dir = 'model/' + model_version\nwith tarfile.open('model.tar.gz', mode='w:gz') as archive:\n    archive.add(export_dir, recursive=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Upload the model to S3\nsagemaker_session = sagemaker.Session()\nmodel_path = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='vis-search/tf/model')\nmodel_path",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Deploy the model in Sagemaker Endpoint. This process will take ~10 min.\nfrom sagemaker.tensorflow import TensorFlowModel\n\nsagemaker_model = TensorFlowModel(\n    model_data=model_path,\n    role=role,\n    framework_version='2.8'\n)\n\npredictor = sagemaker_model.deploy(initial_instance_count=3, instance_type='ml.m5.xlarge')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from io import BytesIO\nimport numpy as np\nimport requests\n\nsm_runtime_client = boto3.client(\"sagemaker-runtime\")\n\n# get the features for a sample image\ndef download_file(url):\n    r = requests.get(url)\n    if r.status_code == 200:\n        file = r.content\n        return file\n    else:\n        print(\"file failed to download\")\n        return None\n    \ndef get_s3_obj(s3_uri):\n    key = s3_uri.replace(f's3://{bucket}/', '')\n    payload = s3.get_object(Bucket=bucket, Key=key)['Body'].read()\n    return payload\n\ndef image_preprocessing(img_bytes, return_body=True):\n    img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n    img = img.resize((224, 224))\n    img = np.asarray(img)\n    img = np.expand_dims(img, axis=0)\n    if return_body:\n        body = json.dumps({\"instances\": img.tolist()})\n        return body\n    else:\n        return img\n    \ndef get_features(img_bytes, sagemaker_endpoint=predictor.endpoint_name):\n    res = image_preprocessing(img_bytes, return_body=True)\n    response = sm_runtime_client.invoke_endpoint(\n        EndpointName=sagemaker_endpoint,\n        ContentType=\"application/json\",\n        Body=res,\n    )\n    response_body = json.loads((response[\"Body\"].read()))\n    features = response_body[\"predictions\"][0]\n    return features\n\nimage_bytes = get_s3_obj('s3://e2eviz-s3buckettraining-1ddugc6fvajd6/data/feidegger/fashion/0000723855b24fbe806c20a1abd9d5dc.jpg?imwidth=400&filter=packshot')\n    \nfeatures = get_features(image_bytes)\nfeatures",
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# return all s3 keys\ndef get_all_s3_keys(bucket):\n    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"    \n    keys = []\n\n    kwargs = {'Bucket': bucket}\n    while True:\n        resp = s3.list_objects_v2(**kwargs)\n        for obj in resp['Contents']:\n            keys.append('s3://' + bucket + '/' + obj['Key'])\n\n        try:\n            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n        except KeyError:\n            break\n\n    return keys",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# get all the zalando images keys from the bucket make a list\ns3_uris = get_all_s3_keys(bucket)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# define a function to extract image features\nfrom time import sleep\n\ndef extract_features(s3_uri):\n    key = s3_uri.replace(f's3://{bucket}/', '')\n    payload = s3.get_object(Bucket=bucket, Key=key)['Body'].read()\n    try:\n        response = get_features(payload)\n    except:\n        sleep(0.1)\n        response = get_features(payload)\n\n    del payload\n    feature_lst = response\n    \n    return s3_uri, feature_lst",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This process cell will take approximately 24-25 minutes on a t3.medium notebook instance\n# with 3 m5.xlarge SageMaker Hosted Endpoint instances\nfrom multiprocessing import cpu_count\nfrom tqdm.contrib.concurrent import process_map\n\nworkers = 2 * cpu_count()\nimg_feature_vectors = process_map(extract_features, s3_uris, max_workers=workers, chunksize=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# setting up the Elasticsearch connection\nfrom opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n\nregion = boto3.Session().region_name # e.g. us-east-1\ncredentials = boto3.Session().get_credentials()\nawsauth = AWSV4SignerAuth(credentials, region)\n\noss = OpenSearch(\n    hosts = [{'host': es_host, 'port': 443}],\n    http_auth = awsauth,\n    use_ssl = True,\n    verify_certs = True,\n    connection_class = RequestsHttpConnection\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Define KNN Elasticsearch index maping\nknn_index = {\n    \"settings\": {\n        \"index.knn\": True\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"zalando_img_vector\": {\n                \"type\": \"knn_vector\",\n                \"dimension\": 2048\n            }\n        }\n    }\n}",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Creating the Elasticsearch index\noss.indices.create(index=\"idx_zalando\",body=knn_index,ignore=400)\noss.indices.get(index=\"idx_zalando\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# defining a function to import the feature vectors corrosponds to each S3 URI into Elasticsearch KNN index\n# This process will take around ~3 min.\n\ndef es_import(elem):\n    oss.index(index='idx_zalando',\n             body={\n                \"zalando_img_vector\": elem[1], \n                \"image\": elem[0]\n             })\n\n_ = process_map(es_import, img_feature_vectors, max_workers=workers, chunksize=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# define display_image function\ndef display_image(bucket, key, size=(300, 300)):\n    response = s3.get_object(Bucket=bucket,Key=key)['Body']\n    img = Image.open(response)\n    img = img.resize(size)\n    return display(img)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import requests\nimport random\n\n\nurls = url_lst[0:10]\n\nimg_bytes = download_file(random.choice(urls))\nfeatures = get_features(img_bytes)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "k = 5\nidx_name = 'idx_zalando'\nres = oss.search(request_timeout=30, index=idx_name,\n                body={'size': k, \n                      'query': {'knn': {'zalando_img_vector': {'vector': features, 'k': k}}}})",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for i in range(k):\n    key = res['hits']['hits'][i]['_source']['image']\n    key = key.replace(f's3://{bucket}/','')\n    img = display_image(bucket, key)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# download ready-made lambda package for backend api\n!aws s3 cp s3://aws-ml-blog/artifacts/visual-search/function.zip ./\n\ns3_resource.Object(bucket, 'backend/function.zip').upload_file('./function.zip')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\nfrom os import environ\n\nnpm_path = ':/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin'\n\nif npm_path not in environ['PATH']:\n    ADD_NPM_PATH = environ['PATH']\n    ADD_NPM_PATH = ADD_NPM_PATH + npm_path\nelse:\n    ADD_NPM_PATH = environ['PATH']\n    \n%set_env PATH=$ADD_NPM_PATH",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "%cd ./frontend/\n\n!npm i --omit=dev",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "!npm run-script build",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "hosting_bucket = f\"s3://{outputs['s3BucketHostingBucketName']}\"\n\n!aws s3 sync ./build/ $hosting_bucket",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "%cd ../",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print('Click the URL below:\\n')\nprint(f'https://{outputs[\"cfDomain\"]}/index.html')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}